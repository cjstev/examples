{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will scrape twitter data through twitter's API and do a tf-idf analysis on that. We will need OAuth authentication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import oauth2 as oauth\n",
    "import urllib2 as urllib\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now need twitter api access. The following steps as available online will help you set up your twitter account and access the live 1% stream.\n",
    "\n",
    "1. Create a twitter account if you do not already have one.\n",
    "2. Go to https://dev.twitter.com/apps and log in with your twitter credentials.\n",
    "3. Click \"Create New App\"\n",
    "4. Fill out the form and agree to the terms. Put in a dummy website if you don't have one you want to use.\n",
    "5. On the next page, click the \"API Keys\" tab along the top, then scroll all the way down until you see the section \"Your Access Token\"\n",
    "6. Click the button \"Create My Access Token\". You can Read more about Oauth authorization online. \n",
    "\n",
    "Save the details of api_key, api_secret, access_token_key, access_token_secret in your vaule directory and load it in the notebook as shown in yelpSample notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/cjstev/Documents/DSE/c1steven/DSE200/DSE200-notebooks/day_5_mining_the_Social_web/')\n",
    "import twitterKeys\n",
    "api_key,api_secret,access_token_key,access_token_secret = twitterKeys.getkeys()\n",
    "\n",
    "_debug = 0\n",
    "\n",
    "oauth_token    = oauth.Token(key=access_token_key, secret=access_token_secret)\n",
    "oauth_consumer = oauth.Consumer(key=api_key, secret=api_secret)\n",
    "\n",
    "signature_method_hmac_sha1 = oauth.SignatureMethod_HMAC_SHA1()\n",
    "\n",
    "http_method = \"GET\"\n",
    "\n",
    "http_handler  = urllib.HTTPHandler(debuglevel=_debug)\n",
    "https_handler = urllib.HTTPSHandler(debuglevel=_debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a twitter request method which will use the above user logins to sign, and open a twitter stream request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTwitterStream(url, method, parameters):\n",
    "    req = oauth.Request.from_consumer_and_token(oauth_consumer,\n",
    "                                             token=oauth_token,\n",
    "                                             http_method=method,\n",
    "                                             http_url=url, \n",
    "                                             parameters=parameters)\n",
    "\n",
    "    req.sign_request(signature_method_hmac_sha1, oauth_consumer, oauth_token)\n",
    "\n",
    "    headers = req.to_header()\n",
    "\n",
    "    if http_method == \"POST\":\n",
    "        encoded_post_data = req.to_postdata()\n",
    "    else:\n",
    "        encoded_post_data = None\n",
    "        url = req.to_url()\n",
    "\n",
    "    opener = urllib.OpenerDirector()\n",
    "    opener.add_handler(http_handler)\n",
    "    opener.add_handler(https_handler)\n",
    "\n",
    "    response = opener.open(url, encoded_post_data)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the above function to request a response as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we will test the above function for a sample data provided by twitter stream here -  \n",
    "url = \"https://stream.twitter.com/1/statuses/sample.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters = []\n",
    "response = getTwitterStream(url, \"GET\", parameters)\n",
    "#response.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function which will take a url and return the top 10 lines returned by the twitter stream\n",
    "\n",
    "** Note ** The response returned needs to be intelligently parsed to get the text data which correspond to actual tweets. This part can be done in a number of ways and you are encouraged to try different approaches to parse the response data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetchData(url,numreturns=10,searchquery='N'):\n",
    "    if searchquery=='N':\n",
    "        top10 = []\n",
    "        response = getTwitterStream(url, \"GET\", parameters)\n",
    "        while len(top10)<numreturns:\n",
    "\n",
    "            testtweet = response.readline()\n",
    "            json_acceptable_string = testtweet.replace(\"'\", \"\\\"\")\n",
    "\n",
    "            try:\n",
    "                tweet = json.loads(json_acceptable_string)\n",
    "                text = tweet['text']\n",
    "                top10.append( tweet['text'])\n",
    "            except:\n",
    "                continue\n",
    "        return top10\n",
    "    else:\n",
    "        response = getTwitterStream(url, \"GET\", parameters)\n",
    "        testtweet = response.readline()\n",
    "        json_acceptable_string = testtweet.replace(\"'\", \"\\\"\")\n",
    "        tweet = json.loads(testtweet)\n",
    "        topx=[]\n",
    "        for i in range(numreturns):\n",
    "            topx.append(tweet['statuses'][i]['text'])\n",
    "        return topx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 RT @madamlesexy: PEKİ AŞKIM BANA BİRAZ GEÇMİŞİNDEN BAHSETSENE, MESELA ESKİ SEVGİLİLERİNLE NELER YAŞADIN https://t.co/iRBC2H7fP1\n",
      "2 RT @neews_sports: الشوط الاول\n",
      "\n",
      "روابط بث مباشر\n",
      "مباراة #الاهلي_الشباب\n",
      "يوتيوب\n",
      "https://t.co/iAb7zz69XD\n",
      "للجوال\n",
      "https://t.co/Ay9xSYAut7\n",
      "\n",
      "رابط مبا…\n",
      "3 RT @slimG_07: 16. a Baltimore classic https://t.co/lOzk3PqWnu\n",
      "4 RT @f14_foo: ❗️❗️عاجل ❗️❗️\n",
      "\n",
      "نشتري القروض العقارية التي صدر لها الموافقة من الصندوق العقاري من جميع المناطق \n",
      "\n",
      "أبونايف : 0553880031\n",
      "5 RT @DepressedDarth: Sticks and stones may break my bones but lightsabers will chop my limbs off\n",
      "6 RT @BookNerdParadis: Discover authors new to you as our guest authors read excerpts at https://t.co/5IBGJyDLpR #books #author #IAN1 #CR4U h…\n",
      "7 RT @qxo1YmpOrKcnXxE: اللهم إني أسألك علماً نافعاً ورزقاً طيباً وعملاً متقبل\n",
      "♻️ https://t.co/wK5C8FUH9W\n",
      "8 RT @kenan_kiran: Yavuz Sultan Selim Köprüsü hayırlı olsun.. https://t.co/UcoT0UBeOe\n",
      "9 RT GreenBison1867 \"RT fahreal_tho: #Howard_RYS16 amartin126: GreenBison1867 vote again. #Howard_RYS16 #Howard_RYS16\" #Howard_RYS16\n",
      "10 @DVennard Blind is a poor defender! Can play a bit but not in Heart of defence. Poor marker.\n"
     ]
    }
   ],
   "source": [
    "tweetlist = fetchData(url)\n",
    "for i in range(len(tweetlist)):\n",
    "    print i+1, tweetlist[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We can also request twitter stream data for specific search parameters as follows\n",
    "#url= \"https://api.twitter.com/1.1/search/tweets.json?q=\"+search_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the fetchData function to fetch latest live stream data for following search queries and output the first 5 lines\n",
    "\n",
    "1. \"UCSD\"\n",
    "2. \"Donald Trump\"\n",
    "3. \"Syria\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3R 2H 0E (WLDC) | UCSD 3, WLDC 6 | End 3rd Recap | https://t.co/BrB1qki7ER\n",
      "2 0R 1H 0E (UCSD) | UCSD 3, WLDC 3 | Mid 3rd Recap | https://t.co/BrB1qki7ER\n",
      "3 @Btrillaaa In LA, Loyola Marymount/USC, uc riverside, then San Diego state and ucsd\n",
      "4 3R 4H 0E (WLDC) | UCSD 3, WLDC 3 | End 2nd Recap | https://t.co/BrB1qki7ER\n",
      "5 RT @Crimbleit: Change Safari’s Default Search Engine in OS X 10 https://t.co/dR1hFmq6tM via #itsupport #college #university #mac #UCLA #UCS…\n"
     ]
    }
   ],
   "source": [
    "search_query = \"UCSD\"\n",
    "url= \"https://api.twitter.com/1.1/search/tweets.json?q=\"+search_query\n",
    "tweetlist = fetchData(url,5,'Y')\n",
    "for i in range(len(tweetlist)):\n",
    "    print i+1, tweetlist[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 RT @YourFavoriteZoe: This happened at a presidential candidate rally... \n",
      "\n",
      "Yup, Donald Trump.  https://t.co/ct702j1UYQ\n",
      "2 Watch 'SNL' Mock Racist Donald Trump Supporters in Campaign Ad https://t.co/Y3dibd0yqa\n",
      "3 On ABC's This Week , Cokie Roberts Says GOP Has Only Itself To Blame For Donald Trump's Rise https://t.co/yHuYFYFCKn\n",
      "4 LOL Chris Wallace vs. Donald Trump https://t.co/Y5lu1wagSA\n",
      "5 RT @quitfeenin: 24. At Donald Trump Rally's: Harmless protestors got their signs ripped and thrown out. #DumpTrump https://t.co/KJzQj6K5ZZ\n"
     ]
    }
   ],
   "source": [
    "search_query = \"Donald Trump\"\n",
    "url= \"https://api.twitter.com/1.1/search/tweets.json?q=\"+search_query\n",
    "tweetlist = fetchData(url,5,'Y')\n",
    "for i in range(len(tweetlist)):\n",
    "    print i+1, tweetlist[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 RT @yo7521: مباراه الشباب والاهلي HD\n",
      "يوتيوب  https://t.co/5yWx8lrQ44   \n",
      "جوال    https://t.co/5yWx8lrQ44   \n",
      "\n",
      "#الشباب_الاهلي\n",
      "https://t.co/KKS…\n",
      "2 RT @yo7521: مباراه الشباب والاهلي HD\n",
      "يوتيوب  https://t.co/5yWx8lrQ44   \n",
      "جوال    https://t.co/5yWx8lrQ44   \n",
      "\n",
      "#الشباب_الاهلي\n",
      "https://t.co/KKS…\n",
      "3 RT @yo7521: مباراه الشباب والاهلي HD\n",
      "يوتيوب  https://t.co/5yWx8lrQ44   \n",
      "جوال    https://t.co/5yWx8lrQ44   \n",
      "\n",
      "#الشباب_الاهلي\n",
      "https://t.co/KKS…\n",
      "4 RT @yo7521: مباراه الشباب والاهلي HD\n",
      "يوتيوب  https://t.co/5yWx8lrQ44   \n",
      "جوال    https://t.co/5yWx8lrQ44   \n",
      "\n",
      "#الشباب_الاهلي\n",
      "https://t.co/KKS…\n",
      "5 RT @yo7521: مباراه الشباب والاهلي HD\n",
      "يوتيوب  https://t.co/5yWx8lrQ44   \n",
      "جوال    https://t.co/5yWx8lrQ44   \n",
      "\n",
      "#الشباب_الاهلي\n",
      "https://t.co/KKS…\n"
     ]
    }
   ],
   "source": [
    "search_query = \"Syria\"\n",
    "url= \"https://api.twitter.com/1.1/search/tweets.json?q=\"+search_query\n",
    "tweetlist = fetchData(url,5,'Y')\n",
    "for i in range(len(tweetlist)):\n",
    "    print i+1, tweetlist[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF###\n",
    "\n",
    "tf–idf, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.It is among the most regularly used statistical tool for word cloud analysis. You can read more about it online (https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "\n",
    "We base our analysis on the following\n",
    "\n",
    "1. The weight of a term that occurs in a document is simply proportional to the term frequency\n",
    "2. The specificity of a term can be quantified as an inverse function of the number of documents in which it occurs\n",
    "\n",
    "For this question we will perform tf-idf analysis o the stream data we retrieve for a given search parameter. Perform the steps below\n",
    "\n",
    "1. use the twitterreq function to search for the query \"syria\" and save the top 200 lines in the file twitterStream.txt\n",
    "2. load the saved file and output the count of occurrences for each term. This will be your term frequency\n",
    "3. Calculate the inverse document frequency for each of the term in the output above.\n",
    "4. Divide the term frequency for each of the term by corresponding inverse document frequency.\n",
    "5. Sort the terms in the descending order based on their term freq/inverse document freq scores \n",
    "6. Print the top 10 terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search_query = \"syria\"   ## chooose query\n",
    "url= \"https://api.twitter.com/1.1/search/tweets.json?lang=en&count=100&q=\"+search_query\n",
    "response = getTwitterStream(url, \"GET\", parameters)\n",
    "testtweet = response.read()\n",
    "tweet = json.loads(testtweet)  ## convert json\n",
    "topx=[]\n",
    "for i in range(100):\n",
    "    topx.append(tweet['statuses'][i]['text']) ## pull tweets into list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topx = [tweet.replace('\\n',' ') for tweet in topx] # put each tweet on one line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outfile = open('twitterStream.txt', 'w')\n",
    "outfile.write(u\"\\n\".join(topx).encode('utf-8').strip()) ## write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "infile = open('twitterStream.txt', 'r')\n",
    "text = infile.read()  #read back file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "exclude = set(string.punctuation)  \n",
    "text = ''.join(ch for ch in text if ch not in exclude)  #drop punctuation\n",
    "text = text.lower()  #convert to lowercase\n",
    "import re\n",
    "word_list=re.split('\\s+',text)  ## split words into wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## create column for number of lines the word appears on\n",
    "numlines = []\n",
    "for word in word_list:\n",
    "    linecount = [word in line for line in text.splitlines()]\n",
    "    numlines.append(sum(linecount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## create column for number of times the word appears overall\n",
    "numtimes = []\n",
    "for word in word_list:\n",
    "    linecount = [word in elem for elem in word_list]\n",
    "    numtimes.append(sum(linecount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## throw it all into a pandas DF\n",
    "import pandas as pd\n",
    "df2 = pd.DataFrame({\"Words\":word_list,\"numlines\":numlines,\"numtimes\":numtimes})\n",
    "df3 = df2.drop_duplicates().sort_values('numlines',ascending=False)  # the way i created this will have duplicates, so drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df3['idf'] = np.log(100/df3['numlines'])  ## create idf col\n",
    "df3['tf'] = np.log(1+df3['numtimes'])  ## create tf col\n",
    "df3['score'] = df3['tf'] * df3['idf']  ## score them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>numlines</th>\n",
       "      <th>numtimes</th>\n",
       "      <th>idf</th>\n",
       "      <th>tf</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>our</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2.813411</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>6.746264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>kurdish</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>2.890372</td>\n",
       "      <td>6.655327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>million</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.605170</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>6.384121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>year</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3.506558</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>6.282908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>hillary</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3.506558</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>6.282908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>my</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2.995732</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>6.229450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>ted</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>2.659260</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>6.123173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>iran</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>2.040221</td>\n",
       "      <td>2.890372</td>\n",
       "      <td>5.896997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>day</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2.813411</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>5.850323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>russia</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>2.659260</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>5.842992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Words  numlines  numtimes       idf        tf     score\n",
       "316       our         6        10  2.813411  2.397895  6.746264\n",
       "33    kurdish        10        17  2.302585  2.890372  6.655327\n",
       "1139  million         1         3  4.605170  1.386294  6.384121\n",
       "1200     year         3         5  3.506558  1.791759  6.282908\n",
       "743   hillary         3         5  3.506558  1.791759  6.282908\n",
       "144        my         5         7  2.995732  2.079442  6.229450\n",
       "1455      ted         7         9  2.659260  2.302585  6.123173\n",
       "176      iran        13        17  2.040221  2.890372  5.896997\n",
       "896       day         6         7  2.813411  2.079442  5.850323\n",
       "294    russia         7         8  2.659260  2.197225  5.842992"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.sort_values('score',ascending=False,inplace=True)\n",
    "df3.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
