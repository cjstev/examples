{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the 20 Newsgroups data set: \n",
    "http://qwone.com/~jason/20Newsgroups/\n",
    "\n",
    "The data consists of  19,000 documents, each from one of 20 newsgroups. \n",
    "\n",
    "Create a naive bayes model that can classify documents based on how often words appear in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/cjstev/Documents/DSE/c1steven/DSE210/20news-bydate 2/matlab\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Documents/DSE/c1steven/DSE210/20news-bydate 2/matlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "from numpy import log\n",
    "from math import isnan\n",
    "from time import time\n",
    "from numpy.random import uniform\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## start timer\n",
    "time1 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Read and Format Data\n",
    "\n",
    "wordlist = pd.read_csv(\"train.data\", header=None, sep=r\"\\s+\")\n",
    "wordlist.columns = ['docIdx', 'wordIdx' ,'counts']\n",
    "label = pd.read_csv(\"train.label\", header=None, sep=r\"\\s+\")\n",
    "maps = pd.read_csv(\"train.map\", header=None, sep=r\"\\s+\")\n",
    "label.reset_index(inplace=True)\n",
    "label['index']=label['index']+1  # to fix indices to rownumbers\n",
    "label.columns = ['docIdx','genreID']\n",
    "maps.columns=['genreName','genreID']\n",
    "vocab = pd.read_csv(\"../../vocabulary.txt\", header=None, sep=r\"\\s+\")\n",
    "vocab.reset_index(inplace=True)\n",
    "vocab['index']=vocab['index']+1  # to fix indices to rownumbers\n",
    "vocab.columns = ['wordIdx','wordName']\n",
    "vocabCount = vocab  ## create a second vocab list that will later be appended for smoothing purposes\n",
    "vocabCount['counts'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Merge DFs\n",
    "mergedDF = wordlist.merge(label).merge(maps).merge(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## split train and validation\n",
    "docselector = pd.DataFrame(mergedDF['docIdx'].drop_duplicates())\n",
    "numpy.random.seed(1)\n",
    "docselector['rand']=uniform(size=len(docselector))\n",
    "mergedDF = mergedDF.merge(docselector)\n",
    "mergedTrain = mergedDF[mergedDF.rand<=.8]\n",
    "mergedValid = mergedDF[mergedDF.rand>.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Calculate pi sub j (probability of each genre occuring)\n",
    "docCount = mergedTrain[['genreID', 'docIdx']].drop_duplicates()\n",
    "docCount = docCount.groupby(['genreID']).count().reset_index()\n",
    "docCount['PIj']=docCount['docIdx']/sum(docCount.docIdx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Calculate baseline train / valid prediction accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create Dictionary of probability distributions of words in genres\n",
    "MNdict = {}\n",
    "for i in mergedTrain.genreID.unique():\n",
    "    byGenre = mergedTrain[mergedTrain['genreID']==i]\n",
    "    byGenre = pd.concat([byGenre[['wordName','wordIdx','counts']],vocabCount[['wordName','wordIdx','counts']]])\n",
    "    byGenre = byGenre.groupby(['wordName','wordIdx']).sum()\n",
    "    byGenre.reset_index(inplace=True)\n",
    "    byGenre['Pj']=byGenre['counts']/sum(byGenre['counts'])\n",
    "    byGenre = byGenre[['wordName','wordIdx','Pj']]\n",
    "    MNdict[i] = byGenre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## convert dictionairy distribution to one dataframe.  bring in PIj to get ready to classify\n",
    "mastertrain = pd.DataFrame()\n",
    "for i in MNdict:\n",
    "    trainset = MNdict[i]\n",
    "    trainset['genreID'] = i\n",
    "    trainset['PIj'] = float(docCount[docCount.genreID==i]['PIj'])\n",
    "    mastertrain=pd.concat([mastertrain,trainset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### classify each document in the testDF\n",
    "bigmerged = pd.merge(mastertrain,mergedValid[['docIdx','wordIdx','counts']])  ## drop actual genre for merging\n",
    "bigmerged['calc']=log(bigmerged['Pj'])+bigmerged['counts']*log(bigmerged['Pj'])\n",
    "bigmerged = bigmerged[['docIdx','genreID','calc']].groupby(['docIdx','genreID']).sum()\n",
    "bigmerged = bigmerged.reset_index()\n",
    "idx = bigmerged.groupby(['docIdx'])['calc'].transform(max) == bigmerged['calc']  ## select argmax for each document\n",
    "testresults = bigmerged[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cjstev/anaconda/lib/python2.7/site-packages/pandas/core/frame.py:2642: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8270985943377549"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check results\n",
    "testresults.rename(columns={'genreID':'predgenre'},inplace=True)\n",
    "testcompare = pd.merge(testresults,mergedValid)\n",
    "sum(testcompare.predgenre == testcompare.genreID)/float(len(testcompare))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Baseline accuracy is 82.7%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test using Log(1+f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create Dictionary of probability distributions of words in genres\n",
    "MNdict = {}\n",
    "for i in mergedTrain.genreID.unique():\n",
    "    byGenre = mergedTrain[mergedTrain['genreID']==i]\n",
    "    byGenre = pd.concat([byGenre[['wordName','wordIdx','counts']],vocabCount[['wordName','wordIdx','counts']]])\n",
    "    byGenre = byGenre.groupby(['wordName','wordIdx']).sum()\n",
    "    byGenre.reset_index(inplace=True)\n",
    "    byGenre['counts'] = log(1+byGenre['counts'])\n",
    "    byGenre['Pj']=byGenre['counts']/sum(byGenre['counts'])\n",
    "    byGenre = byGenre[['wordName','wordIdx','Pj']]\n",
    "    MNdict[i] = byGenre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## convert dictionairy distribution to one dataframe.  bring in PIj to get ready to classify\n",
    "mastertrain = pd.DataFrame()\n",
    "for i in MNdict:\n",
    "    trainset = MNdict[i]\n",
    "    trainset['genreID'] = i\n",
    "    trainset['PIj'] = float(docCount[docCount.genreID==i]['PIj'])\n",
    "    mastertrain=pd.concat([mastertrain,trainset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### classify each document in the testDF\n",
    "bigmerged = pd.merge(mastertrain,mergedValid[['docIdx','wordIdx','counts']])  ## drop actual genre for merging\n",
    "bigmerged['calc']=log(bigmerged['Pj'])+log(1+bigmerged['counts'])*log(bigmerged['Pj'])\n",
    "bigmerged = bigmerged[['docIdx','genreID','calc']].groupby(['docIdx','genreID']).sum()\n",
    "bigmerged = bigmerged.reset_index()\n",
    "idx = bigmerged.groupby(['docIdx'])['calc'].transform(max) == bigmerged['calc']  ## select argmax for each document\n",
    "testresults = bigmerged[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83998713126113644"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check results\n",
    "testresults.rename(columns={'genreID':'predgenre'},inplace=True)\n",
    "testcompare = pd.merge(testresults,mergedValid)\n",
    "sum(testcompare.predgenre == testcompare.genreID)/float(len(testcompare))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Accuracy is slightly boosted to 84.0%, and improvement of 1.3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## split train and validation\n",
    "docselector = pd.DataFrame(mergedDF['docIdx'].drop_duplicates())\n",
    "numpy.random.seed(1)\n",
    "docselector['rand']=uniform(size=len(docselector))\n",
    "mergedDF = mergedDF.merge(docselector)\n",
    "mergedTrain = mergedDF[mergedDF.rand<=.8]\n",
    "mergedValid = mergedDF[mergedDF.rand>.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mergedTrain = mergedTrain[~(mergedTrain['wordName'].isin(stop_words))]\n",
    "mergedValid = mergedValid[~(mergedValid['wordName'].isin(stop_words))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create Dictionary of probability distributions of words in genres\n",
    "MNdict = {}\n",
    "for i in mergedTrain.genreID.unique():\n",
    "    byGenre = mergedTrain[mergedTrain['genreID']==i]\n",
    "    byGenre = pd.concat([byGenre[['wordName','wordIdx','counts']],vocabCount[['wordName','wordIdx','counts']]])\n",
    "    byGenre = byGenre.groupby(['wordName','wordIdx']).sum()\n",
    "    byGenre.reset_index(inplace=True)\n",
    "    byGenre['counts'] = log(1+byGenre['counts'])\n",
    "    byGenre['Pj']=byGenre['counts']/sum(byGenre['counts'])\n",
    "    byGenre = byGenre[['wordName','wordIdx','Pj']]\n",
    "    MNdict[i] = byGenre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## convert dictionairy distribution to one dataframe.  bring in PIj to get ready to classify\n",
    "mastertrain = pd.DataFrame()\n",
    "for i in MNdict:\n",
    "    trainset = MNdict[i]\n",
    "    trainset['genreID'] = i\n",
    "    trainset['PIj'] = float(docCount[docCount.genreID==i]['PIj'])\n",
    "    mastertrain=pd.concat([mastertrain,trainset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### classify each document in the testDF\n",
    "bigmerged = pd.merge(mastertrain,mergedValid[['docIdx','wordIdx','counts']])  ## drop actual genre for merging\n",
    "bigmerged['calc']=log(bigmerged['Pj'])+log(1+bigmerged['counts'])*log(bigmerged['Pj'])\n",
    "bigmerged = bigmerged[['docIdx','genreID','calc']].groupby(['docIdx','genreID']).sum()\n",
    "bigmerged = bigmerged.reset_index()\n",
    "idx = bigmerged.groupby(['docIdx'])['calc'].transform(max) == bigmerged['calc']  ## select argmax for each document\n",
    "testresults = bigmerged[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84334386863458899"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check results\n",
    "testresults.rename(columns={'genreID':'predgenre'},inplace=True)\n",
    "testcompare = pd.merge(testresults,mergedValid)\n",
    "sum(testcompare.predgenre == testcompare.genreID)/float(len(testcompare))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A small boost in accuracy to 84.3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refit new model and test on orig test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docselector = pd.DataFrame(mergedDF['docIdx'].drop_duplicates())\n",
    "mergedDF = mergedDF.merge(docselector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mergedDF = mergedDF[~(mergedDF['wordName'].isin(stop_words))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create Dictionary of probability distributions of words in genres\n",
    "MNdict = {}\n",
    "for i in mergedDF.genreID.unique():\n",
    "    byGenre = mergedDF[mergedDF['genreID']==i]\n",
    "    byGenre = pd.concat([byGenre[['wordName','wordIdx','counts']],vocabCount[['wordName','wordIdx','counts']]])\n",
    "    byGenre = byGenre.groupby(['wordName','wordIdx']).sum()\n",
    "    byGenre.reset_index(inplace=True)\n",
    "    byGenre['counts'] = log(1+byGenre['counts'])\n",
    "    byGenre['Pj']=byGenre['counts']/sum(byGenre['counts'])\n",
    "    byGenre = byGenre[['wordName','wordIdx','Pj']]\n",
    "    MNdict[i] = byGenre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## convert dictionairy distribution to one dataframe.  bring in PIj to get ready to classify\n",
    "mastertrain = pd.DataFrame()\n",
    "for i in MNdict:\n",
    "    trainset = MNdict[i]\n",
    "    trainset['genreID'] = i\n",
    "    trainset['PIj'] = float(docCount[docCount.genreID==i]['PIj'])\n",
    "    mastertrain=pd.concat([mastertrain,trainset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Read in test data\n",
    "testdata = pd.read_csv(\"test.data\", header=None, sep=r\"\\s+\")\n",
    "testdata.columns = ['docIdx', 'wordIdx' ,'counts']\n",
    "testlabel = pd.read_csv(\"test.label\", header=None, sep=r\"\\s+\")\n",
    "testmaps = pd.read_csv(\"test.map\", header=None, sep=r\"\\s+\")\n",
    "testlabel.reset_index(inplace=True)\n",
    "testlabel['index']=testlabel['index']+1\n",
    "testlabel.columns = ['docIdx','genreID']\n",
    "testmaps.columns=['genreName','genreID']\n",
    "testDF = testdata.merge(testlabel).merge(testmaps).merge(vocab[['wordIdx','wordName']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## New features\n",
    "testDF = testDF[~(testDF['wordName'].isin(stop_words))]\n",
    "#testDF['counts']=log(1+testDF['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### classify each document in the testDF\n",
    "bigmerged = pd.merge(mastertrain,testDF[['docIdx','wordIdx','counts']])  ## drop actual genre for merging\n",
    "bigmerged['calc']=log(bigmerged['Pj'])+log(1+bigmerged['counts'])*log(bigmerged['Pj'])\n",
    "bigmerged = bigmerged[['docIdx','genreID','calc']].groupby(['docIdx','genreID']).sum()\n",
    "bigmerged = bigmerged.reset_index()\n",
    "idx = bigmerged.groupby(['docIdx'])['calc'].transform(max) == bigmerged['calc']  ## select argmax for each document\n",
    "testresults = bigmerged[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79886930486425645"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check results\n",
    "testresults.rename(columns={'genreID':'predgenre'},inplace=True)\n",
    "testcompare = pd.merge(testresults,testDF)\n",
    "sum(testcompare.predgenre == testcompare.genreID)/float(len(testcompare))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A small increase in accuracy from 79.4% to 79.9%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
